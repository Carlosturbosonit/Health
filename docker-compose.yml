x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow
  env_file:
    - .env
  environment: &airflow-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"

    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://$AIRFLOW_DB_USER:$AIRFLOW_DB_PASSWORD@airflow-db:5432/$AIRFLOW_DB_NAME

    AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
    AIRFLOW_CONN_POSTGRES_DW: postgresql://$DW_USER:$DW_PASSWORD@postgres_dw:5432/$DW_DB

  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./include:/opt/airflow/include
    - ./data:/data
    - airflow_logs:/opt/airflow/logs
  depends_on:
    airflow-db:
      condition: service_healthy
    postgres_dw:
      condition: service_healthy

services:
  airflow-db:
    image: postgres:15
    container_name: airflow-db
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB_NAME}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER} -d ${AIRFLOW_DB_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 20
    volumes:
      - airflow_db_data:/var/lib/postgresql/data

  postgres_dw:
    image: postgres:15
    container_name: postgres-dw
    environment:
      POSTGRES_USER: ${DW_USER}
      POSTGRES_PASSWORD: ${DW_PASSWORD}
      POSTGRES_DB: ${DW_DB}
    ports:
      - "${DW_PORT_HOST}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DW_USER} -d ${DW_DB}"]
      interval: 5s
      timeout: 5s
      retries: 20
    volumes:
      - dw_data:/var/lib/postgresql/data
      - ./init_db:/docker-entrypoint-initdb.d

  spark-master:
    image: spark:3.5.7-java17
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master", "--port", "7077", "--webui-port", "8080"]
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./data:/data

  spark-worker:
    image: spark:3.5.7-java17
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8081"]
    ports:
      - "8081:8081"
    volumes:
      - ./data:/data
      
  airflow-init:
    <<: *airflow-common
    entrypoint: ["/bin/bash", "-c"]
    command: |
      set -e
      airflow db migrate
      airflow users create \
        --username admin \
        --password admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com || true
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8088:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer
    depends_on:
      airflow-init:
        condition: service_completed_successfully

volumes:
  airflow_db_data:
  dw_data:
  airflow_logs:
