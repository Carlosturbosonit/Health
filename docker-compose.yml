version: "3.8"

services:
  # =========================================================
  # AIRFLOW METADATA DATABASE
  # =========================================================
  airflow-postgres:
    image: postgres:16
    container_name: fluview-airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_pg:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5

  # =========================================================
  # DATA WAREHOUSE POSTGRES
  # =========================================================
  dw-postgres:
    image: postgres:16
    container_name: dw-postgres
    environment:
      POSTGRES_USER: dw
      POSTGRES_PASSWORD: dw
      POSTGRES_DB: dw
    ports:
      - "5434:5432"
    volumes:
      - dw_pg:/var/lib/postgresql/data
      - ./initdb:/docker-entrypoint-initdb.d:ro

  # =========================================================
  # SPARK MASTER
  # =========================================================
  spark-master:
    image: apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.master.Master
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./data:/data
      - ./jars:/jars

  # =========================================================
  # SPARK WORKER
  # =========================================================
  spark-worker:
    image: apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    volumes:
      - ./data:/data
      - ./jars:/jars

  # =========================================================
  # AIRFLOW INIT
  # =========================================================
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: fluview-airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/jars
    entrypoint: /bin/bash
    command: >
      -c "
      airflow db migrate &&
      airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@example.com || true
      "

  # =========================================================
  # AIRFLOW WEBSERVER
  # =========================================================
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: fluview-airflow-webserver
    depends_on:
      - airflow-init
      - spark-master
      - dw-postgres
    environment:
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8088
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
      SPARK_MASTER_URL: spark://spark-master:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/jars
    ports:
      - "8088:8080"
    command: webserver

  # =========================================================
  # AIRFLOW SCHEDULER
  # =========================================================
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: fluview-airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8088
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
      SPARK_MASTER_URL: spark://spark-master:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/jars
    command: scheduler

# =========================================================
# VOLUMES
# =========================================================
volumes:
  airflow_pg:
  dw_pg:

